// Vitest Snapshot v1, https://vitest.dev/guide/snapshot.html

exports[`InlinePromptNode > CHAT_MESSAGE block type > basic > getNodeDisplayFile for CHAT_MESSAGE block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > basic > getNodeFile for CHAT_MESSAGE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import (
    PlainTextPromptBlock,
    VariablePromptBlock,
    RichTextPromptBlock,
    ChatMessagePromptBlock,
    PromptParameters,
)
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        ChatMessagePromptBlock(
            chat_role="SYSTEM",
            blocks=[
                RichTextPromptBlock(
                    blocks=[
                        PlainTextPromptBlock(
                            state="ENABLED",
                            cache_config=None,
                            text="""\\
Summarize the following text:

\\
""",
                        ),
                        VariablePromptBlock(input_variable="text"),
                    ]
                )
            ],
        ),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > legacy prompt variant > getNodeDisplayFile for CHAT_MESSAGE block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > legacy prompt variant > getNodeFile for CHAT_MESSAGE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import (
    PlainTextPromptBlock,
    VariablePromptBlock,
    RichTextPromptBlock,
    ChatMessagePromptBlock,
    PromptParameters,
)
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        ChatMessagePromptBlock(
            chat_role="SYSTEM",
            blocks=[
                RichTextPromptBlock(
                    blocks=[
                        PlainTextPromptBlock(
                            state="ENABLED",
                            cache_config=None,
                            text="""\\
Summarize the following text:

\\
""",
                        ),
                        VariablePromptBlock(input_variable="text"),
                    ]
                )
            ],
        ),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > reject on error enabled > getNodeDisplayFile for CHAT_MESSAGE block type 1`] = `
"from vellum_ee.workflows.display.nodes import (
    BaseInlinePromptNodeDisplay,
    BaseTryNodeDisplay,
)
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


@BaseTryNodeDisplay.wrap(error_output_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"))
class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > reject on error enabled > getNodeFile for CHAT_MESSAGE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.nodes.core import TryNode
from vellum import (
    PlainTextPromptBlock,
    VariablePromptBlock,
    RichTextPromptBlock,
    ChatMessagePromptBlock,
    PromptParameters,
)
from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        ChatMessagePromptBlock(
            chat_role="SYSTEM",
            blocks=[
                RichTextPromptBlock(
                    blocks=[
                        PlainTextPromptBlock(
                            state="ENABLED",
                            cache_config=None,
                            text="""\\
Summarize the following text:

\\
""",
                        ),
                        VariablePromptBlock(input_variable="text"),
                    ]
                )
            ],
        ),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > basic > getNodeDisplayFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > basic > getNodeFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from ..inputs import Inputs
from vellum import FunctionDefinition, PromptParameters


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    prompt_inputs = {
        "text": Inputs.text,
    }
    functions = [
        FunctionDefinition(name="functionTest", description="This is a test function"),
    ]
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > legacy prompt variant > getNodeDisplayFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > legacy prompt variant > getNodeFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from ..inputs import Inputs
from vellum import FunctionDefinition, PromptParameters


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    prompt_inputs = {
        "text": Inputs.text,
    }
    functions = [
        FunctionDefinition(name="functionTest", description="This is a test function"),
    ]
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > reject on error enabled > getNodeDisplayFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum_ee.workflows.display.nodes import (
    BaseInlinePromptNodeDisplay,
    BaseTryNodeDisplay,
)
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


@BaseTryNodeDisplay.wrap(error_output_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"))
class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > reject on error enabled > getNodeFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.nodes.core import TryNode
from ..inputs import Inputs
from vellum import FunctionDefinition, PromptParameters


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    prompt_inputs = {
        "text": Inputs.text,
    }
    functions = [
        FunctionDefinition(name="functionTest", description="This is a test function"),
    ]
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > JINJA block type > basic > getNodeDisplayFile for JINJA block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > JINJA block type > basic > getNodeFile for JINJA block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import JinjaPromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > JINJA block type > legacy prompt variant > getNodeDisplayFile for JINJA block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > JINJA block type > legacy prompt variant > getNodeFile for JINJA block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import JinjaPromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > JINJA block type > reject on error enabled > getNodeDisplayFile for JINJA block type 1`] = `
"from vellum_ee.workflows.display.nodes import (
    BaseInlinePromptNodeDisplay,
    BaseTryNodeDisplay,
)
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


@BaseTryNodeDisplay.wrap(error_output_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"))
class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > JINJA block type > reject on error enabled > getNodeFile for JINJA block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.nodes.core import TryNode
from vellum import JinjaPromptBlock, PromptParameters
from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > basic > getNodeDisplayFile for RICH_TEXT block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > basic > getNodeFile for RICH_TEXT block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import PlainTextPromptBlock, RichTextPromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        RichTextPromptBlock(
            blocks=[
                PlainTextPromptBlock(
                    state="ENABLED", cache_config=None, text="""Hello World!"""
                )
            ]
        ),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > legacy prompt variant > getNodeDisplayFile for RICH_TEXT block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > legacy prompt variant > getNodeFile for RICH_TEXT block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import PlainTextPromptBlock, RichTextPromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        RichTextPromptBlock(
            blocks=[
                PlainTextPromptBlock(
                    state="ENABLED", cache_config=None, text="""Hello World!"""
                )
            ]
        ),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > reject on error enabled > getNodeDisplayFile for RICH_TEXT block type 1`] = `
"from vellum_ee.workflows.display.nodes import (
    BaseInlinePromptNodeDisplay,
    BaseTryNodeDisplay,
)
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


@BaseTryNodeDisplay.wrap(error_output_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"))
class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > reject on error enabled > getNodeFile for RICH_TEXT block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.nodes.core import TryNode
from vellum import PlainTextPromptBlock, RichTextPromptBlock, PromptParameters
from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        RichTextPromptBlock(
            blocks=[
                PlainTextPromptBlock(
                    state="ENABLED", cache_config=None, text="""Hello World!"""
                )
            ]
        ),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > basic > getNodeDisplayFile for VARIABLE block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > basic > getNodeFile for VARIABLE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import VariablePromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        VariablePromptBlock(input_variable="text"),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > legacy prompt variant > getNodeDisplayFile for VARIABLE block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > legacy prompt variant > getNodeFile for VARIABLE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import VariablePromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        VariablePromptBlock(input_variable="text"),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > reject on error enabled > getNodeDisplayFile for VARIABLE block type 1`] = `
"from vellum_ee.workflows.display.nodes import (
    BaseInlinePromptNodeDisplay,
    BaseTryNodeDisplay,
)
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


@BaseTryNodeDisplay.wrap(error_output_id=UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f"))
class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > reject on error enabled > getNodeFile for VARIABLE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.nodes.core import TryNode
from vellum import VariablePromptBlock, PromptParameters
from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        VariablePromptBlock(input_variable="text"),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=[],
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
"
`;

exports[`InlinePromptNode > should generate prompt parameters correctly 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import JinjaPromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(template="""Summarize what this means {{ INPUT_VARIABLE }}"""),
    ]
    prompt_inputs = {
        "text": Inputs.text,
    }
    parameters = PromptParameters(
        stop=["foo", "bar"],
        temperature=0.12,
        max_tokens=345,
        top_p=0.67,
        top_k=8,
        frequency_penalty=0.9,
        presence_penalty=0.11,
        logit_bias={
            "foo": 0.1,
            "bar": 0.2,
        },
        custom_parameters={
            "foo": "bar",
        },
    )
"
`;
